{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ITQML1WOu-5q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN9sDZzsvDlK",
        "outputId": "4bc802bf-cfd5-418c-f33e-fe8ffd3731a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "normalization = transforms.Normalize(mean=normalization_mean, std=normalization_std)"
      ],
      "metadata": {
        "id": "NQImpNm-vHAj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    if max_size:\n",
        "        size = max(image.size)\n",
        "        if size > max_size:\n",
        "            size = max_size\n",
        "            image = image.resize((size, size), Image.ANTIALIAS)\n",
        "    if shape:\n",
        "        image = image.resize(shape, Image.LANCZOS)\n",
        "    if transform:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "    return image.to(device)"
      ],
      "metadata": {
        "id": "R_RS_p6nvJA4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mask(image_path):\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Unable to load image from {image_path}\")\n",
        "\n",
        "    # Apply thresholding to create a binary mask\n",
        "    _, mask = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Convert mask to RGB format\n",
        "    mask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    return mask_rgb"
      ],
      "metadata": {
        "id": "V-5PeNAXvK6y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mask(content_image, mask):\n",
        "    return cv2.bitwise_and(content_image, mask)"
      ],
      "metadata": {
        "id": "qYNzvgkdvYfq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_image_path = '/content/tshirt-content.jpg'\n",
        "style_image_path = '/content/tshirt-style-blue.jpg'\n",
        "mask_image_path = '/content/masked.jpg'"
      ],
      "metadata": {
        "id": "Ho_yG5htvb6S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),  # Resize the image to 256x256 pixels\n",
        "    transforms.CenterCrop(224),  # Crop the center 224x224 pixels\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
        "])"
      ],
      "metadata": {
        "id": "PixzNKHc2ty5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyW-KVzP3MPD",
        "outputId": "97679280-e4b8-41fd-b3c2-1db8b92189c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'tshirt-style-blue.jpg', 'tshirt-content.jpg', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = load_image(content_image_path, transform=transform, max_size=512)\n",
        "style_image = load_image(style_image_path, transform=transform, shape=[content_image.size(2), content_image.size(3)])\n",
        "mask = generate_mask(content_image_path)\n",
        "masked_content_image = apply_mask(content_image_np, resized_mask)\n",
        "cv2.imwrite('/content/masked.jpg', masked_content_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhFwVRnFvdxc",
        "outputId": "edc48ddd-7991-4582-a38a-6e1ae4c670cd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f0e64eccbae6>:7: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize((size, size), Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(content_image_np), content_image_np.shape, content_image_np.dtype)\n",
        "print(type(resized_mask), resized_mask.shape, resized_mask.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_TU_bT67B2T",
        "outputId": "d9b56922-b556-46e7-f7bb-bdf3ca6fcee8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> (224, 224, 3) uint8\n",
            "<class 'numpy.ndarray'> (224, 224, 3) uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert content_image_np.shape == mask.shape, \"Dimensions of content_image_np and mask must match\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "mwy_l2jy7qVC",
        "outputId": "f518d9fb-26fa-4b24-8f5c-33b790a96469"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Dimensions of content_image_np and mask must match",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4b7757ef1860>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mcontent_image_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dimensions of content_image_np and mask must match\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Dimensions of content_image_np and mask must match"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_image_np = content_image.detach().cpu().numpy()\n",
        "content_image_np = np.transpose(content_image_np.squeeze(), (1, 2, 0))  # Assuming NHWC format\n",
        "\n",
        "content_image_np = (content_image_np * 255).astype(np.uint8)\n",
        "\n",
        "resized_mask = cv2.resize(mask, (224, 224))"
      ],
      "metadata": {
        "id": "LqXEeFYM7V0Z"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = models.vgg19(pretrained=True).features\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "vgg.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDKuSr1xxJ2A",
        "outputId": "8802c454-e7ea-47a1-e549-17f6e7106385"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:12<00:00, 45.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (17): ReLU(inplace=True)\n",
              "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (24): ReLU(inplace=True)\n",
              "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (26): ReLU(inplace=True)\n",
              "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (31): ReLU(inplace=True)\n",
              "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (33): ReLU(inplace=True)\n",
              "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (35): ReLU(inplace=True)\n",
              "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentLoss(torch.nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = torch.nn.functional.mse_loss(input, self.target)\n",
        "        return input"
      ],
      "metadata": {
        "id": "1ZPesvq-xMzl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleLoss(torch.nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = self.gram_matrix(target_feature).detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = self.gram_matrix(input)\n",
        "        self.loss = torch.nn.functional.mse_loss(G, self.target)\n",
        "        return input\n",
        "\n",
        "    def gram_matrix(self, input):\n",
        "        a, b, c, d = input.size()\n",
        "        features = input.view(a * b, c * d)\n",
        "        G = torch.mm(features, features.t())\n",
        "        return G.div(a * b * c * d)"
      ],
      "metadata": {
        "id": "eCj72oLuxO18"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
        "                               content_img, style_img, content_layers, style_layers):\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "    model = torch.nn.Sequential(normalization)\n",
        "\n",
        "    i = 0\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, torch.nn.Conv2d):\n",
        "            i += 1\n",
        "            name = 'conv_{}'.format(i)\n",
        "        elif isinstance(layer, torch.nn.ReLU):\n",
        "            name = 'relu_{}'.format(i)\n",
        "            layer = torch.nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, torch.nn.MaxPool2d):\n",
        "            name = 'pool_{}'.format(i)\n",
        "        elif isinstance(layer, torch.nn.BatchNorm2d):\n",
        "            name = 'bn_{}'.format(i)\n",
        "        else:\n",
        "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "\n",
        "    model = model[:(i + 1)]\n",
        "    return model, content_losses, style_losses"
      ],
      "metadata": {
        "id": "F8G4Mw3AxQsM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(torch.nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = mean.clone().detach().view(-1, 1, 1)\n",
        "        self.std = std.clone().detach().view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std"
      ],
      "metadata": {
        "id": "H8ZIqP2X2Hae"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_img = content_image\n",
        "style_img = style_image\n",
        "\n",
        "content_layers = ['conv_4']\n",
        "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "input_img = content_img.clone()\n",
        "optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "model, content_losses, style_losses = get_style_model_and_losses(vgg, normalization_mean, normalization_std,\n",
        "                                                                 content_img, style_img, content_layers, style_layers)"
      ],
      "metadata": {
        "id": "UPN1tnqm2IFd"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Style intensity control\n",
        "style_weight = 1e6  # Adjust this dynamically\n",
        "content_weight = 1"
      ],
      "metadata": {
        "id": "8vM-a_2q2Nfp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "run = [0]\n",
        "while run[0] <= 300:\n",
        "    def closure():\n",
        "        input_img.data.clamp_(0, 1)\n",
        "        optimizer.zero_grad()\n",
        "        model(input_img)\n",
        "        style_score = 0\n",
        "        content_score = 0\n",
        "        for sl in style_losses:\n",
        "            style_score += sl.loss\n",
        "        for cl in content_losses:\n",
        "            content_score += cl.loss\n",
        "\n",
        "        loss = style_score * style_weight + content_score * content_weight\n",
        "        loss.backward()\n",
        "        run[0] += 1\n",
        "        return style_score + content_score\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "input_img.data.clamp_(0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JclV-Ao2ST3",
        "outputId": "ad9e3fa6-03d3-4418-83fb-10d1890e780d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 1., 0.,  ..., 0., 0., 1.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [0., 0., 0.,  ..., 1., 1., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = input_img.cpu().clone().squeeze(0)\n",
        "output = transforms.ToPILImage()(output)\n",
        "output.save('output_image.jpg')"
      ],
      "metadata": {
        "id": "FBactugb2VUN"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}